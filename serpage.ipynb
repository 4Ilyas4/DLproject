{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b2d27f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl \n",
    "import os \n",
    "from typing import List, Dict, Set, Optional\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4b8bdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./data\\ProductionUnit.csv...\n",
      "Loading ./data\\Specification.csv...\n",
      "Loading ./data\\SpecificationSerialRange.csv...\n",
      "Loading ./data\\CatalogTreeNode.csv...\n",
      "Loading ./data\\CatalogTreeNode_CatalogTreeNode.csv...\n",
      "Loading ./data\\VinLockCache.csv...\n",
      "Loading ./data\\ServicePartsPage.csv...\n",
      "Loading ./data\\Section.csv...\n",
      "Loading ./data\\Specification_ProductionUnit.csv...\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data\"\n",
    "\n",
    "def load_data(data_dir: str) -> Dict[str, pl.DataFrame]:\n",
    "    data = {}\n",
    "    tables = [\n",
    "            \"ProductionUnit\",\n",
    "            \"Specification\",\n",
    "            \"SpecificationSerialRange\",\n",
    "            \"CatalogTreeNode\",\n",
    "            \"CatalogTreeNode_CatalogTreeNode\",\n",
    "            \"VinLockCache\",\n",
    "            \"ServicePartsPage\",\n",
    "            \"Section\",\n",
    "            \"Specification_ProductionUnit\"\n",
    "    ]\n",
    "\n",
    "    for table in tables:\n",
    "        file_path = os.path.join(data_dir, f\"{table}.csv\") # Используйте f-строку для удобства\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Loading {file_path}...\") # Можно добавить лог для отладки\n",
    "            # Use infer_schema_length=10000 to better detect column types\n",
    "            data[table] = pl.scan_csv(file_path, infer_schema_length=10000)\n",
    "        else:\n",
    "            print(f\"File not found: {file_path}\") # Добавьте сообщение, если файл отсутствует\n",
    "\n",
    "\n",
    "    return data \n",
    "\n",
    "data = load_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6532a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PartsSearch:\n",
    "   def __init__(self, data: Dict[str, pl.LazyFrame]):\n",
    "      self.data = data\n",
    "      self.catalog_tree_nodes = data[\"CatalogTreeNode\"].collect()\n",
    "      self.catalog_tree_relations = data[\"CatalogTreeNode_CatalogTreeNode\"].collect()\n",
    "      print(f\"Loaded LazyFrames for {len(data)} tables\")\n",
    "      for name, lazy_df in data.items():\n",
    "         row_count = lazy_df.select(pl.count()).collect().item()\n",
    "         print(f\"  - {name}: {row_count} rows\")\n",
    "\n",
    "   def find_specifications_by_production_unit(self, production_unit_id: str) -> pl.DataFrame:\n",
    "      spec_ids = self.data[\"Specification_ProductionUnit\"].filter(\n",
    "          pl.col(\"ProductionUnitID\") == production_unit_id\n",
    "      ).select(\"SpecificationID\").collect()\n",
    "   \n",
    "      if len(spec_ids) == 0:\n",
    "         return pl.DataFrame()\n",
    "      return self.data[\"Specification\"].filter(\n",
    "         pl.col(\"SpecificationID\").is_in(spec_ids[\"SpecificationID\"])\n",
    "      ).collect()\n",
    "\n",
    "   def find_specifications_by_serial_range(self, serial_number: str) -> pl.DataFrame:\n",
    "      matching_ranges = self.data[\"SpecificationSerialRange\"].filter(\n",
    "          (pl.col(\"RangeStart\") <= serial_number) & \n",
    "          (pl.col(\"RangeEnd\") >= serial_number)\n",
    "      ).select(\"SpecificationID\").collect() # No\n",
    "\n",
    "      if len(matching_ranges) == 0:\n",
    "            return pl.DataFrame()\n",
    "      return self.data[\"Specification\"].filter(\n",
    "         pl.col(\"SpecificationID\").is_in(matching_ranges[\"SpecificationID\"])\n",
    "      ).collect()\n",
    "\n",
    "   def find_catalog_nodes_by_serial(self, serial_number: str) -> pl.DataFrame:\n",
    "      node_ids = self.data[\"VinLockCache\"].filter(\n",
    "          str(pl.col(\"SerialNumber\")) == serial_number\n",
    "      ).select(\"CatalogTreeNodeID\").collect()\n",
    "      \n",
    "      if len(node_ids) == 0:\n",
    "         return pl.DataFrame()\n",
    "      return self.catalog_tree_nodes.filter(\n",
    "         pl.col(\"CatalogTreeNodeID\").is_in(node_ids[\"CatalogTreeNodeID\"])\n",
    "      )\n",
    "   \n",
    "   def find_sections_by_nodes(self, catalog_nodes: pl.DataFrame) -> pl.DataFrame:\n",
    "      if len(catalog_nodes) == 0 or \"SectionID\" not in catalog_nodes.columns:\n",
    "         return pl.DataFrame()\n",
    "      section_ids = catalog_nodes.filter(\n",
    "         pl.col(\"NodeType\") == \"Section\"\n",
    "      ).select(\"SectionID\").collect()\n",
    "      if len(section_ids) == 0:\n",
    "         return pl.DataFrame()\n",
    "      return self.data[\"Section\"].filter(\n",
    "         pl.col(\"SectionID\").is_in(section_ids[\"SectionID\"])\n",
    "      ).collect()\n",
    "\n",
    "   def find_service_parts_pages(self,\n",
    "                                  serial_number: str, \n",
    "                                  catalog_nodes: pl.DataFrame) -> pl.DataFrame:  \n",
    "      if len(catalog_nodes) == 0:\n",
    "         return pl.DataFrame()\n",
    "      \n",
    "      direct_pages = catalog_nodes.filter(\n",
    "         pl.col(\"NodeType\") == \"ServicePartsPage\"\n",
    "      )\n",
    "      service_page_ids = set()\n",
    "      if len(direct_pages) > 0 and \"BaseObjectID\" in direct_pages.columns:\n",
    "         service_page_ids.update(direct_pages[\"BaseObjectID\"].to_list())\n",
    "      node_ids = catalog_nodes[\"CatalogTreeNodeID\"].to_list()\n",
    "      print(type(node_ids))\n",
    "      child_pages = self._find_service_parts_pages_recursive(node_ids, set(node_ids))\n",
    "      if len(child_pages) > 0 and \"BaseObjectID\" in child_pages.columns:\n",
    "          service_page_ids.update(child_pages[\"BaseObjectID\"].to_list())\n",
    "\n",
    "      if not service_page_ids:\n",
    "         return pl.DataFrame()\n",
    "      \n",
    "      return self.data[\"ServicePartsPage\"].filter(\n",
    "            pl.col(\"ServicePartsPageID\").is_in(list(service_page_ids))\n",
    "         ).collect()\n",
    "\n",
    "   def _find_service_parts_pages_recursive(self,\n",
    "                                          parent_ids: List[str],\n",
    "                                          visited: Set[str]) -> pl.DataFrame:\n",
    "      if not parent_ids:\n",
    "            return pl.DataFrame()\n",
    "      child_relations = self.catalog_tree_relations.filter(\n",
    "            pl.col(\"Parents_CatalogTreeNodeID\").is_in(parent_ids)\n",
    "      )\n",
    "\n",
    "      if len(child_relations) == 0:\n",
    "            return pl.DataFrame()\n",
    "\n",
    "      child_ids = [id for id in child_relations[\"Children_CatalogTreeNodeID\"].to_list() if id not in visited]\n",
    "\n",
    "      if not child_ids:\n",
    "            return pl.DataFrame()\n",
    "\n",
    "      visited.update(child_ids)\n",
    "\n",
    "      child_nodes = self.catalog_tree_nodes.filter(\n",
    "            pl.col(\"CatalogTreeNodeID\").is_in(child_ids)\n",
    "      )\n",
    "        \n",
    "      service_pages = child_nodes.filter(\n",
    "            pl.col(\"NodeType\") == \"ServicePartsPage\"\n",
    "      )\n",
    "          \n",
    "      deeper_pages = self._find_service_parts_pages_recursive(child_ids, visited)\n",
    " \n",
    "      if len(deeper_pages) > 0:\n",
    "            if len(service_pages) > 0:\n",
    "                  return pl.concat([service_pages, deeper_pages])\n",
    "            return deeper_pages\n",
    "      return service_pages\n",
    "\n",
    "   def process_serial_number(self, serial_number: str) -> Dict:\n",
    "      result = {\n",
    "                \"serialNumber\": serial_number,\n",
    "                \"productionUnit\": None,\n",
    "                \"specifications\": [],\n",
    "                \"sections\": [],\n",
    "                \"servicePartsPages\": []\n",
    "         }\n",
    "\n",
    "      production_unit = self.data[\"ProductionUnit\"].filter(\n",
    "               pl.col(\"SerialNumber\") == serial_number\n",
    "         ).collect()\n",
    "\n",
    "      if len(production_unit) == 0:\n",
    "            return result\n",
    "         \n",
    "      result[\"productionUnit\"] = production_unit[0].to_dict()\n",
    "      production_unit_id = production_unit[0, \"ProductionUnitID\"]\n",
    "\n",
    "      specifications = self.find_specifications_by_production_unit(production_unit_id)\n",
    " \n",
    "      if len(specifications) == 0:\n",
    "            specifications = self.find_specifications_by_serial_range(serial_number)\n",
    "\n",
    "      if len(specifications) > 0:\n",
    "            result[\"specifications\"] = specifications.rows(named=True)\n",
    "      catalog_nodes = self.find_catalog_nodes_by_serial(serial_number)\n",
    "      sections = self.find_sections_by_nodes(catalog_nodes)\n",
    "      if len(sections) > 0:\n",
    "            result[\"sections\"] = sections.rows(named=True)\n",
    "\n",
    "      service_pages = self.find_service_parts_pages(serial_number, catalog_nodes)\n",
    "      if len(service_pages) > 0:\n",
    "                result[\"servicePartsPages\"] = service_pages.rows(named=True)\n",
    "\n",
    "      return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5587599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded LazyFrames for 9 tables\n",
      "  - ProductionUnit: 1039625 rows\n",
      "  - Specification: 4491 rows\n",
      "  - SpecificationSerialRange: 6586 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2236\\826495275.py:8: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "  row_count = lazy_df.select(pl.count()).collect().item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - CatalogTreeNode: 1221033 rows\n",
      "  - CatalogTreeNode_CatalogTreeNode: 1223582 rows\n",
      "  - VinLockCache: 66052 rows\n",
      "  - ServicePartsPage: 271287 rows\n",
      "  - Section: 24009 rows\n",
      "  - Specification_ProductionUnit: 924458 rows\n",
      "Processing 1000 serial numbers...\n"
     ]
    }
   ],
   "source": [
    "search = PartsSearch(data)\n",
    "serial_numbers = data[\"ProductionUnit\"].select(\"SerialNumber\").collect().to_series().to_list()\n",
    "serial_numbers = serial_numbers[:1000] \n",
    "print(f\"Processing {len(serial_numbers)} serial numbers...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f28d773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10/1000 serial numbers (1.57 seconds)\n",
      "Processed 20/1000 serial numbers (3.26 seconds)\n",
      "Processed 30/1000 serial numbers (4.80 seconds)\n",
      "Processed 40/1000 serial numbers (6.27 seconds)\n",
      "Processed 50/1000 serial numbers (7.74 seconds)\n",
      "Processed 60/1000 serial numbers (9.24 seconds)\n",
      "Processed 70/1000 serial numbers (10.71 seconds)\n",
      "Processed 80/1000 serial numbers (12.16 seconds)\n",
      "Processed 90/1000 serial numbers (13.62 seconds)\n",
      "Processed 100/1000 serial numbers (15.07 seconds)\n",
      "Processed 110/1000 serial numbers (16.52 seconds)\n",
      "Processed 120/1000 serial numbers (17.95 seconds)\n",
      "Processed 130/1000 serial numbers (19.36 seconds)\n",
      "Processed 140/1000 serial numbers (20.79 seconds)\n",
      "Processed 150/1000 serial numbers (22.22 seconds)\n",
      "Processed 160/1000 serial numbers (23.64 seconds)\n",
      "Processed 170/1000 serial numbers (25.06 seconds)\n",
      "Processed 180/1000 serial numbers (26.51 seconds)\n",
      "Processed 190/1000 serial numbers (28.01 seconds)\n",
      "Processed 200/1000 serial numbers (29.45 seconds)\n",
      "Processed 210/1000 serial numbers (30.90 seconds)\n",
      "Processed 220/1000 serial numbers (32.36 seconds)\n",
      "Processed 230/1000 serial numbers (33.81 seconds)\n",
      "Processed 240/1000 serial numbers (35.27 seconds)\n",
      "Processed 250/1000 serial numbers (36.75 seconds)\n",
      "Processed 260/1000 serial numbers (38.20 seconds)\n",
      "Processed 270/1000 serial numbers (39.62 seconds)\n",
      "Processed 280/1000 serial numbers (41.03 seconds)\n",
      "Processed 290/1000 serial numbers (42.46 seconds)\n",
      "Processed 300/1000 serial numbers (43.85 seconds)\n",
      "Processed 310/1000 serial numbers (45.28 seconds)\n",
      "Processed 320/1000 serial numbers (46.71 seconds)\n",
      "Processed 330/1000 serial numbers (48.10 seconds)\n",
      "Processed 340/1000 serial numbers (49.57 seconds)\n",
      "Processed 350/1000 serial numbers (51.03 seconds)\n",
      "Processed 360/1000 serial numbers (52.46 seconds)\n",
      "Processed 370/1000 serial numbers (53.91 seconds)\n",
      "Processed 380/1000 serial numbers (55.35 seconds)\n",
      "Processed 390/1000 serial numbers (56.81 seconds)\n",
      "Processed 400/1000 serial numbers (58.30 seconds)\n",
      "Processed 410/1000 serial numbers (59.74 seconds)\n",
      "Processed 420/1000 serial numbers (61.11 seconds)\n",
      "Processed 430/1000 serial numbers (62.56 seconds)\n",
      "Processed 440/1000 serial numbers (64.00 seconds)\n",
      "Processed 450/1000 serial numbers (65.43 seconds)\n",
      "Processed 460/1000 serial numbers (66.88 seconds)\n",
      "Processed 470/1000 serial numbers (68.28 seconds)\n",
      "Processed 480/1000 serial numbers (69.65 seconds)\n",
      "Processed 490/1000 serial numbers (71.10 seconds)\n",
      "Processed 500/1000 serial numbers (72.54 seconds)\n",
      "Processed 510/1000 serial numbers (73.99 seconds)\n",
      "Processed 520/1000 serial numbers (75.40 seconds)\n",
      "Processed 530/1000 serial numbers (77.07 seconds)\n",
      "Processed 540/1000 serial numbers (78.88 seconds)\n",
      "Processed 550/1000 serial numbers (80.65 seconds)\n",
      "Processed 560/1000 serial numbers (82.39 seconds)\n",
      "Processed 570/1000 serial numbers (84.12 seconds)\n",
      "Processed 580/1000 serial numbers (85.90 seconds)\n",
      "Processed 590/1000 serial numbers (87.67 seconds)\n",
      "Processed 600/1000 serial numbers (89.43 seconds)\n",
      "Processed 610/1000 serial numbers (91.16 seconds)\n",
      "Processed 620/1000 serial numbers (92.84 seconds)\n",
      "Processed 630/1000 serial numbers (94.52 seconds)\n",
      "Processed 640/1000 serial numbers (96.24 seconds)\n",
      "Processed 650/1000 serial numbers (97.84 seconds)\n",
      "Processed 660/1000 serial numbers (99.20 seconds)\n",
      "Processed 670/1000 serial numbers (100.54 seconds)\n",
      "Processed 680/1000 serial numbers (101.93 seconds)\n",
      "Processed 690/1000 serial numbers (103.27 seconds)\n",
      "Processed 700/1000 serial numbers (104.60 seconds)\n",
      "Processed 710/1000 serial numbers (105.97 seconds)\n",
      "Processed 720/1000 serial numbers (107.33 seconds)\n",
      "Processed 730/1000 serial numbers (108.69 seconds)\n",
      "Processed 740/1000 serial numbers (110.07 seconds)\n",
      "Processed 750/1000 serial numbers (111.45 seconds)\n",
      "Processed 760/1000 serial numbers (112.84 seconds)\n",
      "Processed 770/1000 serial numbers (114.20 seconds)\n",
      "Processed 780/1000 serial numbers (115.57 seconds)\n",
      "Processed 790/1000 serial numbers (116.92 seconds)\n",
      "Processed 800/1000 serial numbers (118.31 seconds)\n",
      "Processed 810/1000 serial numbers (119.68 seconds)\n",
      "Processed 820/1000 serial numbers (121.09 seconds)\n",
      "Processed 830/1000 serial numbers (122.47 seconds)\n",
      "Processed 840/1000 serial numbers (123.83 seconds)\n",
      "Processed 850/1000 serial numbers (125.26 seconds)\n",
      "Processed 860/1000 serial numbers (126.64 seconds)\n",
      "Processed 870/1000 serial numbers (128.03 seconds)\n",
      "Processed 880/1000 serial numbers (129.40 seconds)\n",
      "Processed 890/1000 serial numbers (130.75 seconds)\n",
      "Processed 900/1000 serial numbers (132.12 seconds)\n",
      "Processed 910/1000 serial numbers (133.51 seconds)\n",
      "Processed 920/1000 serial numbers (134.89 seconds)\n",
      "Processed 930/1000 serial numbers (136.25 seconds)\n",
      "Processed 940/1000 serial numbers (137.62 seconds)\n",
      "Processed 950/1000 serial numbers (138.99 seconds)\n",
      "Processed 960/1000 serial numbers (140.35 seconds)\n",
      "Processed 970/1000 serial numbers (141.74 seconds)\n",
      "Processed 980/1000 serial numbers (143.14 seconds)\n",
      "Processed 990/1000 serial numbers (144.54 seconds)\n",
      "Processed 1000/1000 serial numbers (145.92 seconds)\n",
      "Finished processing 1000 serial numbers in 145.92 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "results = []\n",
    "for i, serial_number in enumerate(serial_numbers):\n",
    "    result = search.process_serial_number(serial_number)\n",
    "    results.append(result)\n",
    "\n",
    "    if (i + 1) % 10 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Processed {i + 1}/{len(serial_numbers)} serial numbers ({elapsed:.2f} seconds)\")\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"Finished processing {len(serial_numbers)} serial numbers in {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51bc604d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>SerialNumber</th><th>HasProductionUnit</th><th>SpecificationCount</th><th>SectionCount</th><th>ServicePartsPageCount</th></tr><tr><td>str</td><td>bool</td><td>i64</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;2201439&quot;</td><td>true</td><td>13</td><td>0</td><td>0</td></tr><tr><td>&quot;807250&quot;</td><td>true</td><td>1</td><td>0</td><td>0</td></tr><tr><td>&quot;2276814&quot;</td><td>true</td><td>6</td><td>0</td><td>0</td></tr><tr><td>&quot;966370&quot;</td><td>true</td><td>1</td><td>0</td><td>0</td></tr><tr><td>&quot;2333032&quot;</td><td>true</td><td>7</td><td>0</td><td>0</td></tr><tr><td>&quot;810230&quot;</td><td>true</td><td>1</td><td>0</td><td>0</td></tr><tr><td>&quot;338637&quot;</td><td>true</td><td>1</td><td>0</td><td>0</td></tr><tr><td>&quot;485420&quot;</td><td>true</td><td>2</td><td>0</td><td>0</td></tr><tr><td>&quot;892076&quot;</td><td>true</td><td>1</td><td>0</td><td>0</td></tr><tr><td>&quot;2341207&quot;</td><td>true</td><td>40</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 5)\n",
       "┌──────────────┬───────────────────┬────────────────────┬──────────────┬───────────────────────┐\n",
       "│ SerialNumber ┆ HasProductionUnit ┆ SpecificationCount ┆ SectionCount ┆ ServicePartsPageCount │\n",
       "│ ---          ┆ ---               ┆ ---                ┆ ---          ┆ ---                   │\n",
       "│ str          ┆ bool              ┆ i64                ┆ i64          ┆ i64                   │\n",
       "╞══════════════╪═══════════════════╪════════════════════╪══════════════╪═══════════════════════╡\n",
       "│ 2201439      ┆ true              ┆ 13                 ┆ 0            ┆ 0                     │\n",
       "│ 807250       ┆ true              ┆ 1                  ┆ 0            ┆ 0                     │\n",
       "│ 2276814      ┆ true              ┆ 6                  ┆ 0            ┆ 0                     │\n",
       "│ 966370       ┆ true              ┆ 1                  ┆ 0            ┆ 0                     │\n",
       "│ 2333032      ┆ true              ┆ 7                  ┆ 0            ┆ 0                     │\n",
       "│ 810230       ┆ true              ┆ 1                  ┆ 0            ┆ 0                     │\n",
       "│ 338637       ┆ true              ┆ 1                  ┆ 0            ┆ 0                     │\n",
       "│ 485420       ┆ true              ┆ 2                  ┆ 0            ┆ 0                     │\n",
       "│ 892076       ┆ true              ┆ 1                  ┆ 0            ┆ 0                     │\n",
       "│ 2341207      ┆ true              ┆ 40                 ┆ 0            ┆ 0                     │\n",
       "└──────────────┴───────────────────┴────────────────────┴──────────────┴───────────────────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_data = {\n",
    "    \"SerialNumber\": [],\n",
    "    \"HasProductionUnit\": [],\n",
    "    \"SpecificationCount\": [],\n",
    "    \"SectionCount\": [],\n",
    "    \"ServicePartsPageCount\": []\n",
    "    }\n",
    "\n",
    "for result in results:\n",
    "    summary_data[\"SerialNumber\"].append(result[\"serialNumber\"])\n",
    "    summary_data[\"HasProductionUnit\"].append(result[\"productionUnit\"] is not None)\n",
    "    summary_data[\"SpecificationCount\"].append(len(result[\"specifications\"]))\n",
    "    summary_data[\"SectionCount\"].append(len(result[\"sections\"]))\n",
    "    summary_data[\"ServicePartsPageCount\"].append(len(result[\"servicePartsPages\"]))\n",
    "\n",
    "summary_df = pl.DataFrame(summary_data)\n",
    "summary_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "894ec60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary data exported to parts_search_summary.csv\n",
      "Exported 426 specifications to parts_search_specifications.csv\n"
     ]
    }
   ],
   "source": [
    "summary_df.write_csv(\"parts_search_summary.csv\")\n",
    "print(\"Summary data exported to parts_search_summary.csv\")\n",
    "details = {\n",
    "\"specifications\": [],\n",
    "\"sections\": [],\n",
    "\"servicePartsPages\": []\n",
    "}\n",
    "\n",
    "for result in results:\n",
    "    serial_number = result[\"serialNumber\"]\n",
    "    for spec in result[\"specifications\"]:\n",
    "        spec[\"SerialNumber\"] = serial_number\n",
    "        details[\"specifications\"].append(spec)\n",
    "\n",
    "    for section in result[\"sections\"]:\n",
    "        section[\"SerialNumber\"] = serial_number\n",
    "        details[\"sections\"].append(section)\n",
    "\n",
    "    for page in result[\"servicePartsPages\"]:\n",
    "        page[\"SerialNumber\"] = serial_number\n",
    "        details[\"servicePartsPages\"].append(page)\n",
    "\n",
    "for key, items in details.items():\n",
    "    if items:\n",
    "        pl.DataFrame(items).write_csv(f\"parts_search_{key}.csv\")\n",
    "        print(f\"Exported {len(items)} {key} to parts_search_{key}.csv\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3908acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any \n",
    "\n",
    "def extract_string_from_series_object(item: Any) -> str | None:\n",
    "    \"\"\"\n",
    "    Извлекает строку из элемента колонки типа Object,\n",
    "    который, предположительно, содержит Polars Series со строкой внутри.\n",
    "    \"\"\"\n",
    "    # Проверяем, является ли элемент Polars Series и не пуст ли он\n",
    "    if isinstance(item, pl.Series) and len(item) > 0:\n",
    "        # Пытаемся получить значение из внутренней Series.\n",
    "        # Предполагаем, что внутренняя Series содержит только один элемент (как в вашем head() выводе).\n",
    "        inner_value = item[0]\n",
    "        # Преобразуем полученное значение в строку, если оно не None\n",
    "        return str(inner_value) if inner_value is not None else None\n",
    "    elif item is None:\n",
    "        # Обрабатываем явные значения None\n",
    "        return None\n",
    "    else:\n",
    "        # Обрабатываем любые другие неожиданные типы данных -\n",
    "        # можно вернуть None, попробовать str(), или вызвать ошибку\n",
    "        # print(f\"Warning: Unexpected item type in Object column: {type(item)}, value: {item}\") # Можно добавить лог\n",
    "        try:\n",
    "             return str(item) # Попытка стандартного преобразования\n",
    "        except Exception:\n",
    "             return None\n",
    "        \n",
    "comprehensive_data = []\n",
    "for result in results:\n",
    "    serial_number = result[\"serialNumber\"]\n",
    "    production_unit = result[\"productionUnit\"]\n",
    "    if not result[\"servicePartsPages\"] and not result[\"specifications\"]:\n",
    "        row = {\n",
    "                \"SerialNumber\": serial_number,\n",
    "                \"ProductionUnitID\": production_unit[\"ProductionUnitID\"] if production_unit else None,\n",
    "                \"SpecificationID\": None,\n",
    "                \"SectionID\": None,\n",
    "                \"ServicePartsPageID\": None\n",
    "        }\n",
    "        comprehensive_data.append(row)\n",
    "        continue\n",
    "\n",
    "    for page in result[\"servicePartsPages\"]:\n",
    "        for spec in result[\"specifications\"]:\n",
    "            for section in result[\"sections\"] if result[\"sections\"] else [None]:\n",
    "                row = {\n",
    "                      \"SerialNumber\": serial_number,\n",
    "                        \"ProductionUnitID\": production_unit[\"ProductionUnitID\"] if production_unit else None,\n",
    "                        \"SpecificationID\": spec[\"SpecificationID\"],\n",
    "                        \"SectionID\": section[\"SectionID\"] if section else None,\n",
    "                        \"ServicePartsPageID\": page[\"ServicePartsPageID\"]\n",
    "                    }\n",
    "                comprehensive_data.append(row)\n",
    "        \n",
    "        if result[\"specifications\"] and not result[\"servicePartsPages\"]:\n",
    "            for spec in result[\"specifications\"]:\n",
    "                row = {\n",
    "                    \"SerialNumber\": serial_number,\n",
    "                    \"ProductionUnitID\": production_unit[\"ProductionUnitID\"] if production_unit else None,\n",
    "                    \"SpecificationID\": spec[\"SpecificationID\"],\n",
    "                    \"SectionID\": None,\n",
    "                    \"ServicePartsPageID\": None\n",
    "                }\n",
    "                comprehensive_data.append(row)\n",
    "    \n",
    "    if comprehensive_data:\n",
    "        # Убедитесь, что comprehensive_data - это список словарей,\n",
    "        # где значение для ProductionUnitID - это сама строка UUID, а не Series\n",
    "        # Если comprehensive_data создается таким образом, что внутри попадают Series,\n",
    "        # то вам нужно исправить ЭТО место.\n",
    "        # Например:\n",
    "        # comprehensive_data = []\n",
    "        # for ...: # Ваш цикл сбора данных\n",
    "        #    row = { ... 'ProductionUnitID': actual_uuid_string, ... } # Здесь должна быть строка, а не Series\n",
    "        #    comprehensive_data.append(row)\n",
    "\n",
    "\n",
    "        comprehensive_df = pl.DataFrame(comprehensive_data)\n",
    "\n",
    "        # print(\"Original Schema:\")\n",
    "        # print(comprehensive_df.schema)\n",
    "\n",
    "        # --- ПРИМЕНЯЕМ ИСПРАВЛЕНИЕ К КОЛОНКЕ 'ProductionUnitID' ---\n",
    "        try:\n",
    "            # Используем map_elements для применения функции к каждому элементу колонки\n",
    "            # Указываем return_dtype=pl.String, так как функция возвращает строку или None\n",
    "            comprehensive_df = comprehensive_df.with_columns(\n",
    "                pl.col(\"ProductionUnitID\").map_elements(\n",
    "                    extract_string_from_series_object, return_dtype=pl.String\n",
    "                ).alias(\"ProductionUnitID\") # Сохраняем исходное имя колонки\n",
    "            )\n",
    "            # print(\"\\nSchema after processing 'ProductionUnitID':\")\n",
    "            # print(comprehensive_df.schema)\n",
    "\n",
    "            # --- УДАЛИТЕ ЭТУ СТРОКУ! --- Вы повторно создавали DataFrame, отбрасывая исправления.\n",
    "            # comprehensive_df = pl.DataFrame(comprehensive_data,) # !!! УДАЛИТЬ !!!\n",
    "            # --- КОНЕЦ УДАЛЕНИЯ ---\n",
    "\n",
    "            # --- ПРИМЕНЯЕМ ПРЕОБРАЗОВАНИЕ ТИПОВ ДЛЯ ОСТАЛЬНЫХ OBJECT-КОЛОНОК, ЕСЛИ ЕСТЬ ---\n",
    "            # Пройдемся по всем колонкам после обработки ProductionUnitID\n",
    "            for col_name, col_dtype in comprehensive_df.schema.items():\n",
    "                if col_dtype == pl.Object:\n",
    "                    # print(f\"Attempting to cast remaining object column '{col_name}' to String.\")\n",
    "                    # # Попытка стандартного приведения для других object-колонок, если они есть\n",
    "                    comprehensive_df = comprehensive_df.with_columns(\n",
    "                        pl.col(col_name).cast(pl.String).alias(col_name) # Приведение и сохранение имени\n",
    "                    )\n",
    "                    # print(f\"Column '{col_name}' schema after cast attempt: {comprehensive_df.schema[col_name]}\")\n",
    "\n",
    "\n",
    "            # --- ТЕПЕРЬ ЗАПИСЫВАЕМ В CSV ---\n",
    "            # print(\"\\nAttempting to write to CSV...\")\n",
    "            comprehensive_df.write_csv(\"C:/Users/user/Desktop/parts_search_comprehensive.csv\")\n",
    "            # print(f\"Exported comprehensive dataset with {len(comprehensive_df)} rows to parts_search_comprehensive.csv\")\n",
    "            # print(comprehensive_df.head(10))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn error occurred during processing or writing: {e}\")\n",
    "            print(\"Please review the data source, the extraction function, and column names.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
